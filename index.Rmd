---
title: "Practical Machine Learning Project"
author: "Lina"
date: "17 de junio de 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load('Results.RData')
```

## Definition of the project

The objective of the project is to predict how well weigth
lifting exercise is performed using a dataset from 
http://groupware.les.inf.puc-rio.br/har. The dataset consists
of a sample of 19,622 observations with 159 features related.
The response variable (classe) is a categorical variable 
with 5 levels.

## Feature selection

I kept only the numerical variables with no missing observations.
I also removed the raw_timestamp_part_1, raw_timestamp_part_2
and num_window variables. My final dataset consists of 52 variables 
in addition the response variable.

## Data partitioning

The dataset was split into 3 subsets:
- Training dataset (60%)
- Testing dataset (20%)
- Validation set (20%)

## Cross-validation

I used K-fold cross-validation with 5 folds.

## Models

Using the caret package, I trained two models: 
Random Forests and Gradient Boosting Machines

## Results

### 1. Random Forests

Confussion matrix:

```{r, echo=FALSE}
tab_rf
```
Test accuracy:
```{r, echo=FALSE}
acc_rf
```
### 2. Gradient Boosting Machines

Confussion matrix:

```{r, echo=FALSE}
tab_gbm
```
Test accuracy:
```{r, echo=FALSE}
acc_gbm
```

## Best model

The resuls from the Random Forests model are slightly better than those
from the Gradient Boosting Machines. 
The more important variables in the prediction are roll_belt and yaw_belt.

Variables importance with Random Forests:
```{r, echo=FALSE}
fig
```

## Expected out of sample error

The accuracy of the Random Forests model in the validation set is:

```{r, echo=FALSE}
acc_out
```
