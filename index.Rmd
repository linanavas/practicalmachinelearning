---
title: "Practical Machine Learning Project"
author: "Lina Navas"
date: "17 de junio de 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
options(warn=-1)
```

## Definition of the project

The objective of the project is to predict how well weigth
lifting exercise is performed using a dataset from 
http://groupware.les.inf.puc-rio.br/har. The dataset consists
of a sample of 19,622 observations with 159 features related.
The response variable (classe) is a categorical variable 
with 5 levels.

## Feature selection

I kept only the numerical variables with no missing observations.
I also removed the raw_timestamp_part_1, raw_timestamp_part_2
and num_window variables. My final dataset consists of 52 variables 
in addition the response variable.

```{r, echo=TRUE}
# Loading data
plm_train <- read.csv('pml-training.csv')
plm_test <- read.csv('pml-testing.csv')
# Cleaning data
classe <- plm_train %>% 
  select(X, classe)
plm_train <- plm_train %>% 
  select(-classe) %>% 
  mutate_if(is.factor, function(x)as.numeric(as.character(x))) %>% 
  select_if(~!any(is.na(.))) %>% 
  left_join(classe, by = 'X') %>% 
  select(-raw_timestamp_part_1, -raw_timestamp_part_2,-X,-num_window)
```

## Data partitioning

The dataset was split into 3 subsets:
- Training dataset (60%)
- Testing dataset (20%)
- Validation set (20%)

```{r, echo=TRUE}
# Spliting data

set.seed(1234)
inTrain <- createDataPartition(plm_train %>% pull(classe), p = 0.6, list = F)
training <- plm_train[inTrain,]
noTraining <- plm_train[-inTrain,]
inTest <- createDataPartition(noTraining %>% pull(classe), p = 0.5, list = F)
testing <- noTraining[inTest,]
validation <- noTraining[-inTest,]

```

## Cross-validation

I used K-fold cross-validation with 5 folds.

```{r, echo=TRUE}
# Parameter to be used in the train function
ctrl <- trainControl(method = 'cv', number = 5, verboseIter = FALSE)
```

## Models

Using the caret package, I trained two models: 
Random Forests and Gradient Boosting Machines

```{r, echo=TRUE}
# Training models
set.seed(5678)
ctrl <- trainControl(method = 'cv', number = 5)
model_rf <- train(classe ~ ., method = 'rf',data = training,trControl = ctrl)
model_gbm <- train(classe ~ ., method = 'gbm',data = training,trControl = ctrl)
```

## Results

### 1. Random Forests

```{r, echo=TRUE}
pred_rf <- predict(model_rf, testing)
matrix_rf <- confusionMatrix(testing$classe, pred_rf)
tab_rf <- matrix_rf$table
acc_rf <- matrix_rf$overall[1]
```
Confussion matrix:
```{r, echo=FALSE}
tab_rf
```
Test accuracy:
```{r, echo=FALSE}
acc_rf
```
### 2. Gradient Boosting Machines

```{r, echo=TRUE}
pred_gbm <- predict(model_gbm, testing)
matrix_gbm <- confusionMatrix(testing$classe, pred_gbm)
tab_gbm <- matrix_gbm$table
acc_gbm <- matrix_gbm$overall[1]
```
Confussion matrix:
```{r, echo=FALSE}
tab_gbm
```
Test accuracy:
```{r, echo=FALSE}
acc_gbm
```

## Best model

The resuls from the Random Forests model are better than those
from the Gradient Boosting Machines. 
The most important variables in the prediction are roll_belt and yaw_belt.

```{r, echo=TRUE}
# Var importance
imp_rf <- varImp(model_rf)$importance
```

Variables importance with Random Forests:
```{r, echo=FALSE}
imp_rf <- imp_rf %>% 
  mutate(Variable = rownames(.)) %>% 
  arrange(desc(Overall)) %>% 
  filter(row_number()<=10)

fig <- ggplot(aes(y = Variable, x = Overall), data = imp_rf %>% 
         mutate(Variable = factor(Variable, levels = imp_rf %>% 
                                    arrange(Overall) %>% 
                                    pull(Variable)))) + 
  geom_bar(stat = 'identity', fill = 'cornflowerblue') +
  xlab('Importance') + 
  ylab('Feature') + 
  theme_classic()
fig
```

## Expected out of sample error

The accuracy of the Random Forests model in the validation set is:

```{r, echo=TRUE}
# Validation
pred_out <- predict(model_rf, validation)
matrix_out <- confusionMatrix(validation$classe, pred_out)
acc_out <- matrix_out$overall[1]
```

```{r, echo=FALSE}
acc_out
```
## Test sample prediction
The prediction for the testing dataset using the Random Forests model is:

```{r, echo=TRUE}
predict(model_rf, plm_test)
```